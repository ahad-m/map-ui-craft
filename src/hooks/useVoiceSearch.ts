/**
 * Voice Search Hook
 * 
 * Custom React hook for implementing voice search functionality using the Web Speech API.
 * Provides speech recognition for Arabic language with proper error handling and user feedback.
 * 
 * Features:
 * - Arabic speech recognition (ar-SA)
 * - Browser compatibility checking
 * - Microphone permission handling
 * - Real-time listening state
 * - User-friendly error messages
 * 
 * @module hooks/useVoiceSearch
 */

import { useState } from 'react';
import { toast } from '@/hooks/use-toast';

/**
 * Custom hook for voice search functionality
 * 
 * Implements speech-to-text conversion using the Web Speech API.
 * Handles browser compatibility, permissions, and error states.
 * 
 * **Browser Support:**
 * - Chrome/Edge: Full support
 * - Safari: Partial support
 * - Firefox: Not supported (as of 2024)
 * 
 * **Permissions:**
 * Requires microphone access. Users will be prompted on first use.
 * 
 * @param onResult - Callback function that receives the transcribed text
 * @returns Object containing listening state and start function
 * 
 * @example
 * const { isListening, startListening } = useVoiceSearch((transcript) => {
 *   console.log("User said:", transcript);
 *   setSearchQuery(transcript);
 * });
 * 
 * // In component JSX
 * <Button onClick={startListening} disabled={isListening}>
 *   {isListening ? "Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹..." : "Ø¨Ø­Ø« ØµÙˆØªÙŠ"}
 * </Button>
 */
export const useVoiceSearch = (onResult: (transcript: string) => void) => {
  // Track whether speech recognition is currently active
  const [isListening, setIsListening] = useState(false);

  /**
   * Start voice recognition process
   * 
   * Initializes the speech recognition service and handles all stages:
   * - Browser compatibility check
   * - Service initialization
   * - Result processing
   * - Error handling
   * - Cleanup
   */
  const startListening = () => {
    // Get SpeechRecognition API (with vendor prefixes for compatibility)
    const SpeechRecognition =
      (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;

    // Check if browser supports speech recognition
    if (!SpeechRecognition) {
      toast({
        title: 'ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…',
        description: 'Ù…ØªØµÙØ­Ùƒ Ù„Ø§ ÙŠØ¯Ø¹Ù… Ù…ÙŠØ²Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ØµÙˆØªÙŠ. Ø¬Ø±Ø¨ Ù…ØªØµÙØ­ Chrome Ø£Ùˆ Edge.',
        variant: 'destructive',
      });
      return;
    }

    // Create new recognition instance
    const recognition = new SpeechRecognition();
    
    // Configure recognition settings
    recognition.lang = 'ar-SA'; // Arabic (Saudi Arabia)
    recognition.continuous = false; // Stop after first result
    recognition.interimResults = false; // Only return final results

    /**
     * Handle recognition start
     * Called when speech recognition service begins listening
     */
    recognition.onstart = () => {
      setIsListening(true);
    };

    /**
     * Handle recognition results
     * Called when speech is successfully recognized
     * 
     * @param event - Speech recognition event containing results
     */
    recognition.onresult = (event: any) => {
      // Extract transcript from first result
      const transcript = event.results[0][0].transcript;
      // Pass transcript to parent component
      onResult(transcript);
    };

    /**
     * Handle no match scenario
     * Called when speech is detected but not understood
     */
    recognition.onnomatch = () => {
      toast({
        title: 'Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…',
        description: 'Ø­Ø§ÙˆÙ„ Ø§Ù„ØªØ­Ø¯Ø« Ø¨ÙˆØ¶ÙˆØ­ Ø£ÙƒØ«Ø±.',
        variant: 'destructive',
      });
    };

    /**
     * Handle recognition errors
     * Provides specific error messages based on error type
     * 
     * @param event - Error event containing error details
     */
    recognition.onerror = (event: any) => {
      if (event.error === 'not-allowed') {
        // Microphone permission denied
        toast({
          title: 'Ø§Ù„Ù…Ø§ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù…Ø­Ø¬ÙˆØ¨',
          description: 'ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø§ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ØªØµÙØ­ (Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ù‚ÙÙ„ ğŸ”’).',
          variant: 'destructive',
        });
      } else {
        // Other recognition errors
        toast({
          title: 'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØµÙˆØª',
          description: `Ø­Ø¯Ø« Ø®Ø·Ø£: ${event.error}. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.`,
          variant: 'destructive',
        });
      }
    };

    /**
     * Handle recognition end
     * Called when speech recognition service stops listening
     * Resets listening state
     */
    recognition.onend = () => {
      setIsListening(false);
    };

    // Attempt to start recognition
    try {
      recognition.start();
    } catch (e) {
      // Handle case where recognition is already running
      setIsListening(false);
      toast({
        title: 'Ø®Ø·Ø£',
        description: 'Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† Ø¨Ø¯Ø¡ Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª. Ù‚Ø¯ ØªÙƒÙˆÙ† Ù‚ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù….',
        variant: 'destructive',
      });
    }
  };

  return {
    /** Whether speech recognition is currently active */
    isListening,
    /** Function to start speech recognition */
    startListening,
  };
};
